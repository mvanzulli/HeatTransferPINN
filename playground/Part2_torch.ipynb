{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9ce379",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb95bd",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial }{\\partial x*} \\left(  ( 1 + \\epsilon_c \\theta )  \\frac{\\partial \\theta }{\\partial x*} \\right)   - N^2 \\theta + N^2 G ( 1 + \\epsilon_G \\theta ) = 0   $$ \n",
    "\n",
    "#### Where: \n",
    "\n",
    "- $\\theta(x) = \\dfrac{T(x) - T_\\inf}{T_{b} - T_{\\inf}}$\n",
    "\n",
    "\n",
    "- $N = \\sqrt{\\dfrac{h P L^2}{ k_0 A }} $\n",
    "\n",
    "\n",
    "- $G=\\dfrac{q_0A}{hP(T_b-T_\\inf)}$\n",
    "\n",
    "\n",
    "- $\\epsilon_G=\\epsilon(T_b-T_\\inf)$\n",
    "\n",
    "\n",
    "- $\\epsilon_C=\\beta(T_b-T_\\inf)$\n",
    "\n",
    "#### and the boundary conditions are:\n",
    "\n",
    "- at $x^* = 0 \\rightarrow \\theta = 1$\n",
    "\n",
    "- at $x^* = 1 \\rightarrow \\dfrac{d\\theta}{dx^*} = 0$\n",
    "\n",
    "\n",
    "**Given a sample which contains 9 values of $x$ and $\\theta$ predict $G$** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcf210",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513fe630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import matplotlib.ticker\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c20f37f",
   "metadata": {},
   "source": [
    "## Set random seed and select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102276aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devuce employed: cpu\n"
     ]
    }
   ],
   "source": [
    "#Set default dtype to float32\n",
    "# torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    \n",
    "device = 'cpu'\n",
    "print('Devuce employed:', device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0cf47",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad72fca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta real shape is: torch.Size([9, 1])\n",
      "X real shape is: (9, 1)\n",
      "G_real is: [[0.21590551]]\n",
      "lb is 0.00 and ub is 1.00\n",
      "The number real points are 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load dataset\n",
    "relative_csv_path = \"./../src/data/raw/Dataset.csv\"\n",
    "split_ratio = 0.2\n",
    "\n",
    "# Load a random sample\n",
    "fields_G = ['G']\n",
    "fields_T = ['T1', 'T2', 'T3', 'T4' ,'T5', 'T6' ,'T7', 'T8' ,'T9']\n",
    "fields = fields_T + fields_G\n",
    "df_T = pd.read_csv(relative_csv_path, skipinitialspace=True, usecols=fields)\n",
    "random_sample = df_T.sample()\n",
    "G_real = random_sample[fields_G].to_numpy()\n",
    "T_real = random_sample[fields_T].to_numpy()\n",
    "X_real = np.linspace(0,1,T_real.shape[1]).reshape(-1,1)\n",
    "# compute theta \n",
    "T_inf = 27 + 273 #in K\n",
    "T_b = 127 + 273 #in K\n",
    "Theta_real = np.multiply(np.add(T_real, [-T_inf]), [1/(T_b - T_inf)]).T\n",
    "Theta_real = torch.from_numpy(Theta_real)\n",
    "# prin shapes \n",
    "print('Theta real shape is:',Theta_real.shape)\n",
    "print('X real shape is:',X_real.shape)\n",
    "print('G_real is:',G_real)\n",
    "\n",
    "# lower and uper bounds to normalize the input of net U\n",
    "lb = min(X_real)\n",
    "ub = max(X_real)\n",
    "print('lb is %.2f and ub is %.2f' % (lb,ub))\n",
    "N_u = Theta_real.shape[0]\n",
    "print(\"The number real points are\", N_u)\n",
    "X_real.dtype\n",
    "Theta_real.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec94d872",
   "metadata": {},
   "source": [
    "### Collocation points ($loss_f$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1f0caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of collocation points are 9\n"
     ]
    }
   ],
   "source": [
    "N_f = 9 #Total number of collocation points \n",
    "f_hat = torch.zeros(N_f,1).to(device)\n",
    "print(\"The number of collocation points are\", N_f)\n",
    "X_train_f = torch.linspace(lb[0],ub[0],N_f).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661c8c7",
   "metadata": {},
   "source": [
    "## Model hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfbfee7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of layers selected are 10\n"
     ]
    }
   ],
   "source": [
    "steps = 20000\n",
    "lr = 1e-3\n",
    "input_dim = X_real.shape[1]\n",
    "output_dim = Theta_real.shape[1]\n",
    "layers = np.array([input_dim,20,20,20,20,20,20,20,20,output_dim]) #number of neurons for each layer\n",
    "print(\"The number of layers selected are\", layers.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d64b7",
   "metadata": {},
   "source": [
    "## Deep NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e943c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Deep Neural Network\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "        'Initialize neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers) - 1)])\n",
    "    \n",
    "        'Xavier Normal Initialization'\n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            \n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "              \n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        # inpunt and hidden layers forward computation\n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = self.linears[i](a)\n",
    "                        \n",
    "            a = self.activation(z)\n",
    "\n",
    "        # output layer forward computation            \n",
    "        a = self.linears[-1](a)\n",
    "        \n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717cdae6",
   "metadata": {},
   "source": [
    "## PINN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b71242dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN():\n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        'Define loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialize iterator'\n",
    "        self.iter = 0\n",
    "        \n",
    "        'Initialize our new parameter G, N, ec, eg as tensor (Inverse problem)' \n",
    "        self.G = torch.tensor([float(G)], requires_grad=True).float().to(device)  \n",
    "        self.N = torch.tensor([float(N)], requires_grad=True).float().to(device)  \n",
    "        self.ec = torch.tensor([float(ec)], requires_grad=True).float().to(device)  \n",
    "        self.eg = torch.tensor([float(eg)], requires_grad=True).float().to(device)  \n",
    "\n",
    "                \n",
    "        'Register parametes to be optimized'\n",
    "        self.G = nn.Parameter(self.G)\n",
    "        self.N= nn.Parameter(self.N)\n",
    "        self.ec= nn.Parameter(self.ec)\n",
    "        self.eg= nn.Parameter(self.eg)\n",
    "\n",
    "        'Initialize our DNN'\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        'Register our new parameter'\n",
    "        self.dnn.register_parameter('G', self.G)  \n",
    "        self.dnn.register_parameter('N', self.N)  \n",
    "        self.dnn.register_parameter('ec', self.ec)  \n",
    "        self.dnn.register_parameter('eg', self.eg)  \n",
    "\n",
    "    def loss_data(self,x,u):\n",
    "        \n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)\n",
    "            \n",
    "        if torch.is_tensor(u) != True:         \n",
    "            u = torch.from_numpy(u)               \n",
    "        \n",
    "        loss_u = self.loss_function(self.dnn(x), u)\n",
    "        \n",
    "        return loss_u\n",
    "    \n",
    "    def loss_PDE(self, X_train_f):\n",
    "        \n",
    "        if torch.is_tensor(X_train_f) != True:         \n",
    "            X_train_f = torch.from_numpy(X_train_f)     \n",
    "                   \n",
    "        # extract the PDE parameters\n",
    "        G = self.G\n",
    "        N = self.N\n",
    "        ec = self.ec\n",
    "        eg = self.eg\n",
    "\n",
    "        # clone the input data and add AD\n",
    "        x = X_train_f.clone()\n",
    "        x.requires_grad = True\n",
    "        \n",
    "        # predict theta\n",
    "        theta = self.dnn(x)\n",
    "\n",
    "        # compute derivatives \n",
    "        theta_x = autograd.grad(theta, x, torch.ones([X_train_f.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "        #theta_x.requires_grad = True \n",
    "        \n",
    "        # copmute term to be derived\n",
    "        aux = (1 + ec * theta) * theta_x\n",
    "        \n",
    "        aux_x = autograd.grad(aux, x, torch.ones(X_train_f.shape).to(device), create_graph=True)[0]\n",
    "                \n",
    "        # PDE f  = 0\n",
    "        f = aux_x - N**2 * theta + N**2 * G * (1 + eg * theta)   \n",
    "\n",
    "        # f_hat is just an auxiliar term to copmute the loss (is zero)\n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,x_u,u,x_f):\n",
    "\n",
    "        loss_u = self.loss_data(x_u, u)\n",
    "        print(x_f.shape)\n",
    "        loss_f = self.loss_PDE(x_f)\n",
    "        \n",
    "        loss_val = loss_u + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(X_real, Theta_real, X_train_f)\n",
    "        \n",
    "        loss.backward()\n",
    "                \n",
    "        self.iter += 1\n",
    "        \n",
    "        if self.iter % 100 == 0:\n",
    "\n",
    "            error_vec, _ = PINN.test()\n",
    "        \n",
    "            print(\n",
    "                'Relative Error(Test): %.5f , G_real = [%.5f], G_PINN = [%.5f]' %\n",
    "                (\n",
    "                    error_vec.cpu().detach().numpy(),\n",
    "                    G,\n",
    "                    self.G.item(),\n",
    "                )\n",
    "            )\n",
    "                \n",
    "\n",
    "        return loss        \n",
    "    \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "                \n",
    "        theta_pred = self.dnn(Theta_real)\n",
    "        \n",
    "        error_vec = torch.linalg.norm((Theta_real - theta_pred),2)/torch.linalg.norm(Theta_real,2)        # Relative L2 Norm of the error (Vector)\n",
    "        \n",
    "        theta_pred = theta_pred.cpu().detach().numpy()\n",
    "                        \n",
    "        return error_vec, theta_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673c41f",
   "metadata": {},
   "source": [
    "<!-- ## Initialize parameters  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cfb393c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Te real G = [ [[0.70303543]] ]. Our initial guess will be G_PINN= [ 0.5 ]\n",
      "Our initial of the other params are: \n",
      " N  = [ 2 ]\n",
      " eg = [ 0.2 ] \n",
      " ec = [ 0.4 ]\n"
     ]
    }
   ],
   "source": [
    "G = .5\n",
    "N = 2\n",
    "ec = .4\n",
    "eg = .2\n",
    "print(\"Te real G = [\",G_real,\"]. Our initial guess will be G_PINN= [\",G,\"]\")\n",
    "print(\"Our initial of the other params are: \\n N  = [\",N,\"]\\n eg = [\",eg,\"] \\n ec = [\",ec,\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75d10f",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1f10fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN(\n",
      "  (activation): Tanh()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (2): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (3): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (5): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (6): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (7): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (8): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor(1.1498, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1498, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([9, 1])\n",
      "torch.Size([9, 1])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m pinn\u001b[38;5;241m.\u001b[39mloss(X_real, Theta_real, X_train_f)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(loss))\n\u001b[0;32m---> 51\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# optimizer.step(pinn.closure)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# print('Test Error: %.5f'  % (error_vec))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "'Neural Network Summary'\n",
    "pinn = PINN(layers)\n",
    "print(pinn.dnn)\n",
    "\n",
    "'Neural Network Parameters'\n",
    "params = list(pinn.dnn.parameters())\n",
    "\n",
    "'L-BFGS Optimizer'\n",
    "optimizer = torch.optim.LBFGS(params, lr, \n",
    "                              max_iter = steps, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = 1e-11, \n",
    "                              tolerance_change = 1e-11, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "'Test loss u methods'\n",
    "y_nn =pinn.dnn(X_real)\n",
    "loss_theta_manufacuted = pinn.loss_function(y_nn, Theta_real)\n",
    "print(loss_theta_manufacuted)\n",
    "loss_theta = pinn.loss_data(X_real, Theta_real)\n",
    "print(loss_theta)\n",
    "assert(loss_theta_manufacuted == loss_theta)\n",
    "\n",
    "'Test loss f methods'\n",
    "loss_pde = pinn.loss_PDE(X_train_f)\n",
    "\n",
    "'Test overeall loss'\n",
    "loss_manufactured = loss_pde + loss_theta\n",
    "overall_loss = pinn.loss(X_real, Theta_real, X_train_f)\n",
    "# print(loss_manufactured.to_numpy())\n",
    "# print(overall_loss)\n",
    "# assert(loss_theta_manufacuted == overall_loss)\n",
    "\n",
    "'L-BFGS Optimizer'\n",
    "optimizer = torch.optim.LBFGS(params, lr, \n",
    "                              max_iter = steps, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = 1e-11, \n",
    "                              tolerance_change = 1e-11, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss = pinn.loss(X_real, Theta_real, X_train_f)\n",
    "\n",
    "print(type(loss))\n",
    "loss.backward()\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# optimizer.step(pinn.closure)\n",
    "    \n",
    "    \n",
    "# elapsed = time.time() - start_time                \n",
    "# print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "\n",
    "# ''' Model Accuracy ''' \n",
    "# error_vec, u_pred = PINN.test()\n",
    "\n",
    "# print('Test Error: %.5f'  % (error_vec))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
